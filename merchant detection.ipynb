{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "import datetime, time, calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x000002402944F860>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join \n",
    "# ip <-> ar <-> mb\n",
    "#           <-> dept\n",
    "\n",
    "# ip <-> ar : \"ip_id\"\n",
    "# ip <-> cc_cst : \"x\"\n",
    "# cc_cst <-> cc_visa : \"x\"\n",
    "# ar <-> mb : \"src_ar_id\"\n",
    "# ar <-> dept : \"ar_id\"\n",
    "\n",
    "# DerivedFeature\n",
    "# [PerWeek & PerMonth]\n",
    "# noTransferInUnique, noTransferOutUnique, noTransferIn, noTransferOut, ratioTransferIn, ratioTransferOut\n",
    "# avgTransferInAmount, avgTransferOutAmount, avgTransferAmount\n",
    "# noFeeAmountGroupBy(0,10,25,35)\n",
    "# noTransferInPeriod(morning,afternoon,evening,night :: 6hr)\n",
    "# noTransferOutPeriod(morning,afternoon,evening,night :: 6hr)\n",
    "# noTransferInPerDay(Mon-Thu, Fri, Sat-Sun [avg]) <-- PerMonth\n",
    "# noTransferOutPerDay(Mon-Thu, Fri, Sat-Sun [avg]) <-- PerMonth\n",
    "# avgTransferInHoliday, avgTransferOutHoliday\n",
    "\n",
    "\n",
    "# Deposit\n",
    "# [PerWeek & PerMonth]\n",
    "# noBranchVisit,noBranchVisitUnique,ratioBranchVisit\n",
    "# avgTransferInAmount, avgTransferOutAmount, avgTransferAmount\n",
    "# noDepositAmount, noWithdrawAmount\n",
    "# avgDepositAmount, avgWithdrawAmount\n",
    "# noWithdrawPeriod(morning,afternoon,evening,night :: 6hr)\n",
    "# noDepositPeriod(morning,afternoon,evening,night :: 6hr)\n",
    "# noDepositPerDay(Mon-Thu, Fri, Sat-Sun [avg]) <-- PerMonth\n",
    "# noWithdrawPerDay(Mon-Thu, Fri, Sat-Sun [avg]) <-- PerMonth\n",
    "# noTransferInUnique, noTransferOutUnique, noTransferIn, noTransferOut, ratioTransferIn, ratioTransferOut\n",
    "# noTransferInPerDay(Mon-Thu, Fri, Sat-Sun [avg]) <-- PerMonth\n",
    "# noTransferOutPerDay(Mon-Thu, Fri, Sat-Sun [avg]) <-- PerMonth\n",
    "# ??Balance??\n",
    "# ??CBS Sub Operation Code??\n",
    "# ahrzd_usr_id <-- [KMP, EDC, ATM]\n",
    "\n",
    "# Credit\n",
    "\n",
    "\n",
    "# Approach\n",
    "# 1 Detech ลูกน้องโดยดูจำนวนเงินที่รับมากที่สุดแล้วเช็ค\n",
    "\n",
    "# Visualize\n",
    "# Location\n",
    "\n",
    "# feature = [\"vc_ip.ip_id\"]\n",
    "# category = [\"vc_ip.ip_tp_cd\",\"vc_ip.mar_st_cd\",\"vc_ip.ctf_tp_cd\",\"vc_ip.ocp_cd\",\"vc_ip.idv_incm_seg_cd\"]\n",
    "# filteredFeature = [\"vc_ip.prvn_f\",\"vc_ip.ip_st_cd\",\"vc_ip.death_f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002402944F358>\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"MerchantInsight\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"/opt/jupyter_workspace/spark-warehouse\")\n",
    "         .getOrCreate())\n",
    "print (spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_trans_data =(spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\",\"true\")\n",
    "      .csv('mock_data/mock-transaction.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+------+------+------+-------+\n",
      "|fm_ar_id|to_ar_id|ptn_yyyy|ptn_mm|ptn_dd|ptn_hr|ptn_min|\n",
      "+--------+--------+--------+------+------+------+-------+\n",
      "|     128|     140|    2016|     6|    19|    11|     22|\n",
      "|     114|     150|    2016|     8|     4|    13|     37|\n",
      "|     131|     120|    2016|     7|     6|    13|     31|\n",
      "|     144|     104|    2016|     9|    10|    11|     32|\n",
      "|     133|     130|    2016|    12|    15|    13|     25|\n",
      "|     109|     140|    2016|     6|    19|    22|     33|\n",
      "|     116|     146|    2016|     7|    23|    15|     57|\n",
      "|     110|     128|    2016|    10|    29|    20|     55|\n",
      "|     101|     123|    2016|     7|    10|    22|     52|\n",
      "|     128|     137|    2016|     8|    30|    20|     38|\n",
      "|     108|     114|    2016|    12|    19|    20|     49|\n",
      "|     103|     142|    2016|    12|    31|    18|      5|\n",
      "|     135|     123|    2016|     8|     3|    14|     52|\n",
      "|     118|     141|    2016|     7|    27|    15|     19|\n",
      "|     138|     145|    2016|     6|    22|    14|     56|\n",
      "|     126|     123|    2016|    11|    25|    21|     39|\n",
      "|     110|     107|    2016|     9|    26|    11|      6|\n",
      "|     118|     106|    2016|     6|    29|    11|     25|\n",
      "|     134|     101|    2016|     8|    18|     8|     28|\n",
      "|     129|     132|    2016|    10|    12|    14|     28|\n",
      "+--------+--------+--------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mb_trans_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pre-processing data\n",
    "\n",
    "def no_days_in_month(month, year):\n",
    "    if month in day_months_31: \n",
    "        return 31\n",
    "    elif month in day_months_30:\n",
    "        return 30\n",
    "    else:\n",
    "        if calendar.isleap(year):\n",
    "            return 29\n",
    "        else:\n",
    "            return 28\n",
    "        \n",
    "def day_of_week_code(day_of_week):\n",
    "    if day_of_week < 4:\n",
    "        return 0\n",
    "    elif day_of_week > 4:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def quarter_code(date, month):\n",
    "    month_31_days = [1,3,5,7,8,10,12]\n",
    "    month_30_days = [4,6,9,11]\n",
    "    if(month in month_31_days):\n",
    "        if(date in range(1,9)):\n",
    "            return 1\n",
    "        elif(date in range(9,16)):\n",
    "            return 2\n",
    "        elif(date in range(16,24)):\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "    elif(month in month_30_days):\n",
    "        if(date in range(1,9)):\n",
    "            return 1\n",
    "        elif(date in range(9,16)):\n",
    "            return 2\n",
    "        elif(date in range(16,23)):\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "    else: # February\n",
    "        return date / 4 \n",
    "\n",
    "def period_code(time):\n",
    "    hour = int(time[:-6])\n",
    "    if hour in range(0, 6):\n",
    "        return 0\n",
    "    elif hour in range(6, 12):\n",
    "        return 1\n",
    "    elif hour in range(12, 18):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "quarter_code_udf = udf(quarter_code,IntegerType())\n",
    "date = udf(lambda y, m, d : datetime.datetime(y, m ,d), DateType())\n",
    "day_of_week = udf(lambda date : int(date.weekday()), IntegerType())\n",
    "day_of_week_code_udf = udf(day_of_week_code, IntegerType())\n",
    "quarter_code_udf = udf(quarter_code, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarter_code(19,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mb_trans_data.select(\"ptn_dd\").collect()[0].ptn_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+------+------+------+-------+------------+----------+-----------+----------------+\n",
      "|fm_ar_id|to_ar_id|ptn_yyyy|ptn_mm|ptn_dd|ptn_hr|ptn_min|quarter_code|      date|day_of_week|day_of_week_code|\n",
      "+--------+--------+--------+------+------+------+-------+------------+----------+-----------+----------------+\n",
      "|     128|     140|    2016|     6|    19|    11|     22|           3|2016-06-19|          6|               2|\n",
      "|     114|     150|    2016|     8|     4|    13|     37|           1|2016-08-04|          3|               0|\n",
      "|     131|     120|    2016|     7|     6|    13|     31|           1|2016-07-06|          2|               0|\n",
      "|     144|     104|    2016|     9|    10|    11|     32|           2|2016-09-10|          5|               2|\n",
      "|     133|     130|    2016|    12|    15|    13|     25|           2|2016-12-15|          3|               0|\n",
      "|     109|     140|    2016|     6|    19|    22|     33|           3|2016-06-19|          6|               2|\n",
      "|     116|     146|    2016|     7|    23|    15|     57|           3|2016-07-23|          5|               2|\n",
      "|     110|     128|    2016|    10|    29|    20|     55|           4|2016-10-29|          5|               2|\n",
      "|     101|     123|    2016|     7|    10|    22|     52|           2|2016-07-10|          6|               2|\n",
      "|     128|     137|    2016|     8|    30|    20|     38|           4|2016-08-30|          1|               0|\n",
      "|     108|     114|    2016|    12|    19|    20|     49|           3|2016-12-19|          0|               0|\n",
      "|     103|     142|    2016|    12|    31|    18|      5|           4|2016-12-31|          5|               2|\n",
      "|     135|     123|    2016|     8|     3|    14|     52|           1|2016-08-03|          2|               0|\n",
      "|     118|     141|    2016|     7|    27|    15|     19|           4|2016-07-27|          2|               0|\n",
      "|     138|     145|    2016|     6|    22|    14|     56|           3|2016-06-22|          2|               0|\n",
      "|     126|     123|    2016|    11|    25|    21|     39|           4|2016-11-25|          4|               1|\n",
      "|     110|     107|    2016|     9|    26|    11|      6|           4|2016-09-26|          0|               0|\n",
      "|     118|     106|    2016|     6|    29|    11|     25|           4|2016-06-29|          2|               0|\n",
      "|     134|     101|    2016|     8|    18|     8|     28|           3|2016-08-18|          3|               0|\n",
      "|     129|     132|    2016|    10|    12|    14|     28|           2|2016-10-12|          2|               0|\n",
      "+--------+--------+--------+------+------+------+-------+------------+----------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mb_trans_data = mb_trans_data.withColumn(\"quarter_code\",quarter_code_udf(mb_trans_data['ptn_dd'],mb_trans_data['ptn_mm']))\n",
    "mb_trans_data = mb_trans_data.withColumn(\"date\", date(mb_trans_data[\"ptn_yyyy\"], mb_trans_data[\"ptn_mm\"], mb_trans_data[\"ptn_dd\"]))\n",
    "mb_trans_data = mb_trans_data.withColumn(\"day_of_week\", day_of_week(mb_trans_data[\"date\"]))\n",
    "mb_trans_data = mb_trans_data.withColumn(\"day_of_week_code\", day_of_week_code_udf(mb_trans_data[\"day_of_week\"]))\n",
    "mb_trans_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarter_code(16,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----+\n",
      "|fm_ar_id|quarter_code|count|\n",
      "+--------+------------+-----+\n",
      "|     101|           1|    1|\n",
      "|     101|           2|    3|\n",
      "|     101|           4|    2|\n",
      "|     102|           1|    1|\n",
      "|     102|           2|    3|\n",
      "|     102|           3|    1|\n",
      "|     102|           4|    4|\n",
      "|     103|           2|    1|\n",
      "|     103|           4|    2|\n",
      "|     104|           2|    1|\n",
      "|     104|           3|    2|\n",
      "|     105|           1|    1|\n",
      "|     105|           4|    3|\n",
      "|     106|           1|    1|\n",
      "|     106|           4|    2|\n",
      "|     107|           1|    1|\n",
      "|     107|           4|    5|\n",
      "|     108|           1|    4|\n",
      "|     108|           2|    2|\n",
      "|     108|           3|    3|\n",
      "+--------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mb_trans_data.select('fm_ar_id').groupby('fm_ar_id').count().show()\n",
    "mb_trans_data.select('fm_ar_id',mb_trans_data['quarter_code']).groupby(['fm_ar_id','quarter_code']).count().sort(['fm_ar_id','quarter_code']).show()\n",
    "# mb_trans_data.groupby(mb_trans_data['quarter_code']).count().show()\n",
    "# F.count(mb_trans_data['quarter_code']==1).alias(\"noMbTransferOutDuringQ1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`fm_ar_id_quarter`' given input columns: [3, fm_ar_id_quarter_code, 2, 4, 1];;\\n'Sort ['fm_ar_id_quarter ASC NULLS FIRST], true\\n+- Project [fm_ar_id_quarter_code#5924, coalesce(1#5925L, cast(0.0 as bigint)) AS 1#5941L, coalesce(2#5926L, cast(0.0 as bigint)) AS 2#5942L, coalesce(3#5927L, cast(0.0 as bigint)) AS 3#5943L, coalesce(4#5928L, cast(0.0 as bigint)) AS 4#5944L]\\n   +- LocalRelation [fm_ar_id_quarter_code#5924, 1#5925L, 2#5926L, 3#5927L, 4#5928L]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mC:\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2139.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`fm_ar_id_quarter`' given input columns: [3, fm_ar_id_quarter_code, 2, 4, 1];;\n'Sort ['fm_ar_id_quarter ASC NULLS FIRST], true\n+- Project [fm_ar_id_quarter_code#5924, coalesce(1#5925L, cast(0.0 as bigint)) AS 1#5941L, coalesce(2#5926L, cast(0.0 as bigint)) AS 2#5942L, coalesce(3#5927L, cast(0.0 as bigint)) AS 3#5943L, coalesce(4#5928L, cast(0.0 as bigint)) AS 4#5944L]\n   +- LocalRelation [fm_ar_id_quarter_code#5924, 1#5925L, 2#5926L, 3#5927L, 4#5928L]\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:280)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:280)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)\r\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:58)\r\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2850)\r\n\tat org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:2838)\r\n\tat org.apache.spark.sql.Dataset.sort(Dataset.scala:1038)\r\n\tat sun.reflect.GeneratedMethodAccessor281.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-24124555c979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# mb_trans_data.select('fm_ar_id','quarter_code').groupby(['fm_ar_id','quarter_code']).agg(F.count(mb_trans_data['quarter_code']==1).alias(\"noMbTransferOutDuringQ1\")).sort('fm_ar_id').show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmb_trans_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fm_ar_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'to_ar_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'quarter_code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmb_trans_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fm_ar_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"quarter_code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fm_ar_id_quarter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ar_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'noMbTransferOutQ1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'noMbTransferOutQ2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'noMbTransferOutQ3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'noMbTransferOutQ4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \"\"\"\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`fm_ar_id_quarter`' given input columns: [3, fm_ar_id_quarter_code, 2, 4, 1];;\\n'Sort ['fm_ar_id_quarter ASC NULLS FIRST], true\\n+- Project [fm_ar_id_quarter_code#5924, coalesce(1#5925L, cast(0.0 as bigint)) AS 1#5941L, coalesce(2#5926L, cast(0.0 as bigint)) AS 2#5942L, coalesce(3#5927L, cast(0.0 as bigint)) AS 3#5943L, coalesce(4#5928L, cast(0.0 as bigint)) AS 4#5944L]\\n   +- LocalRelation [fm_ar_id_quarter_code#5924, 1#5925L, 2#5926L, 3#5927L, 4#5928L]\\n\""
     ]
    }
   ],
   "source": [
    "# mb_trans_data.select('fm_ar_id','quarter_code').groupby(['fm_ar_id','quarter_code']).agg(F.count(mb_trans_data['quarter_code']==1).alias(\"noMbTransferOutDuringQ1\")).sort('fm_ar_id').show()\n",
    "mb_trans_data.select('fm_ar_id','to_ar_id','quarter_code').distinct()\n",
    "mb_trans_data.stat.crosstab(\"fm_ar_id\",\"quarter_code\").sort('fm_ar_id_quarter')\\\n",
    ".toDF('ar_id','noMbTransferOutQ1','noMbTransferOutQ2','noMbTransferOutQ3','noMbTransferOutQ4').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "def getMbFrequency(columnName,newColumnName):\n",
    "    freqIn = mb_trans_data.select(columnName).groupby(columnName).count()\n",
    "    if((columnName == \"fm_ar_id\") | (columnName == \"to_ar_id\")):\n",
    "        freqIn = freqIn.withColumnRenamed(columnName,\"ar_id\")\n",
    "    freqIn = freqIn.withColumnRenamed(\"count\",newColumnName)\n",
    "    return freqIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Extraction V2\n",
    "def getMbFrequencyPerQuarter(columnName,newColumnName):\n",
    "    freqIn = mb_trans_data.stat.crosstab(columnName,\"quarter_code\")\\\n",
    "    .toDF(columnName,newColumnName+'Q1',newColumnName+'Q2',newColumnName+'Q3',newColumnName+'Q4')\n",
    "    if((columnName == \"fm_ar_id\") | (columnName == \"to_ar_id\")):\n",
    "        freqIn = freqIn.withColumnRenamed(columnName,\"ar_id\")\n",
    "    return freqIn\n",
    "\n",
    "def getMbFrequencyUniquePerQuarter(columnName,newColumnName):\n",
    "    mb_trans_data_unique = mb_trans_data.select('fm_ar_id','to_ar_id','quarter_code').distinct()\n",
    "    freqIn = mb_trans_data_unique.stat.crosstab(columnName,\"quarter_code\")\\\n",
    "    .toDF(columnName,newColumnName+'Q1',newColumnName+'Q2',newColumnName+'Q3',newColumnName+'Q4')\n",
    "    if((columnName == \"fm_ar_id\") | (columnName == \"to_ar_id\")):\n",
    "        freqIn = freqIn.withColumnRenamed(columnName,\"ar_id\")\n",
    "    return freqIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ar_id: integer (nullable = true)\n",
      " |-- noMbTransferOutQ1: long (nullable = true)\n",
      " |-- noMbTransferOutQ2: long (nullable = true)\n",
      " |-- noMbTransferOutQ3: long (nullable = true)\n",
      " |-- noMbTransferOutQ4: long (nullable = true)\n",
      " |--  noMbTransferInQ1: long (nullable = true)\n",
      " |--  noMbTransferInQ2: long (nullable = true)\n",
      " |--  noMbTransferInQ3: long (nullable = true)\n",
      " |--  noMbTransferInQ4: long (nullable = true)\n",
      " |--  noMbTransferOutUniqueQ1: long (nullable = true)\n",
      " |--  noMbTransferOutUniqueQ2: long (nullable = true)\n",
      " |--  noMbTransferOutUniqueQ3: long (nullable = true)\n",
      " |--  noMbTransferOutUniqueQ4: long (nullable = true)\n",
      " |--  noMbTransferInUniqueQ1: long (nullable = true)\n",
      " |--  noMbTransferInUniqueQ2: long (nullable = true)\n",
      " |--  noMbTransferInUniqueQ3: long (nullable = true)\n",
      " |--  noMbTransferInUniqueQ4: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = mb_trans_data.select(\"fm_ar_id\").distinct().withColumnRenamed(\"fm_ar_id\",\"ar_id\")\n",
    "train_data = df.join(getMbFrequencyPerQuarter(\"fm_ar_id\",\"noMbTransferOut\"),\"ar_id\",\"left_outer\")\n",
    "train_data = train_data.join(getMbFrequencyPerQuarter(\"to_ar_id\",\" noMbTransferIn\"),\"ar_id\",\"left_outer\")\n",
    "train_data = train_data.join(getMbFrequencyUniquePerQuarter(\"fm_ar_id\",\" noMbTransferOutUnique\"),\"ar_id\",\"left_outer\")\n",
    "train_data = train_data.join(getMbFrequencyUniquePerQuarter(\"to_ar_id\",\" noMbTransferInUnique\"),\"ar_id\",\"left_outer\")\n",
    "train_data.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
